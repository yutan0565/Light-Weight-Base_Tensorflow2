{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yutan0565/colab_git/blob/main/code/MobileNet_V3_Small_%EC%98%88%EC%8B%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTOd0ahJOZsr"
   },
   "source": [
    "##기본 모델 형성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNI1wdG4N7yF",
    "outputId": "2080654c-6979-443e-9516-da8a3f2f40f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |█▍                              | 10 kB 20.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 20 kB 22.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 30 kB 27.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 40 kB 16.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 51 kB 14.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 61 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 71 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 81 kB 14.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 92 kB 15.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 102 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 112 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 122 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 133 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 143 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 153 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 163 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 174 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 184 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 194 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▋    | 204 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 215 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 225 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 235 kB 16.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 237 kB 16.3 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pZRQ2Jc4OCS1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard\n",
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "7R6Yikb4OCuf",
    "outputId": "c2fdef5f-6db2-40a4-c2c4-a76675e44fc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(raw_train_x.shape)\\nprint(raw_test_x.shape)\\nprint(raw_train_y.shape)\\nprint(raw_test_y.shape)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_train_x, raw_train_y), (raw_test_x, raw_test_y) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "\"\"\"\n",
    "print(raw_train_x.shape)\n",
    "print(raw_test_x.shape)\n",
    "print(raw_train_y.shape)\n",
    "print(raw_test_y.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "yuQ_qeDSODzw",
    "outputId": "13f56c8b-6d6b-4024-bf7f-68d821b8e578"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(train_x.shape)\\nprint(valid_x.shape)\\nprint(test_x.shape)\\nprint(train_y.shape)\\nprint(valid_y.shape)\\nprint(test_y.shape)\\n\\ndef show_sample(i):\\n  print(raw_train_y[i][0], labels[raw_train_y[i][0]])\\n  plt.imshow(raw_train_x[i])\\n  plt.show()\\n\\nfor i in [2, 10, 12, 14]:\\n  show_sample(i)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# float을 넣을거면 0~1 사이 값으로 바꿔야함\n",
    "# integer 형태로 넣어보기\n",
    "train_x = raw_train_x[:45000].astype(np.float32)/255.0\n",
    "valid_x = raw_train_x[45000:].astype(np.float32)/255.0\n",
    "test_x = raw_test_x.astype(np.float32)/255.0\n",
    "\n",
    "\n",
    "train_y = raw_train_y[:45000]\n",
    "valid_y = raw_train_y[45000:]\n",
    "test_y = raw_test_y\n",
    "labels = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(train_x.shape)\n",
    "print(valid_x.shape)\n",
    "print(test_x.shape)\n",
    "print(train_y.shape)\n",
    "print(valid_y.shape)\n",
    "print(test_y.shape)\n",
    "\n",
    "def show_sample(i):\n",
    "  print(raw_train_y[i][0], labels[raw_train_y[i][0]])\n",
    "  plt.imshow(raw_train_x[i])\n",
    "  plt.show()\n",
    "\n",
    "for i in [2, 10, 12, 14]:\n",
    "  show_sample(i)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wwn3NFrmOFeq",
    "outputId": "39e134ed-bbd7-4d3b-84b3-18c23da45a40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16711680/16705208 [==============================] - 11s 1us/step\n",
      "16719872/16705208 [==============================] - 11s 1us/step\n",
      "Model: \"efficientnetb0\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "rescaling (Rescaling)           (None, 32, 32, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "normalization (Normalization)   (None, 32, 32, 3)    7           rescaling[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "stem_conv_pad (ZeroPadding2D)   (None, 33, 33, 3)    0           normalization[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "stem_conv (Conv2D)              (None, 16, 16, 32)   864         stem_conv_pad[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "stem_bn (BatchNormalization)    (None, 16, 16, 32)   128         stem_conv[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "stem_activation (Activation)    (None, 16, 16, 32)   0           stem_bn[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1a_dwconv (DepthwiseConv2D (None, 16, 16, 32)   288         stem_activation[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1a_bn (BatchNormalization) (None, 16, 16, 32)   128         block1a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block1a_activation (Activation) (None, 16, 16, 32)   0           block1a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block1a_se_squeeze (GlobalAvera (None, 32)           0           block1a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block1a_se_reshape (Reshape)    (None, 1, 1, 32)     0           block1a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block1a_se_reduce (Conv2D)      (None, 1, 1, 8)      264         block1a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block1a_se_expand (Conv2D)      (None, 1, 1, 32)     288         block1a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block1a_se_excite (Multiply)    (None, 16, 16, 32)   0           block1a_activation[0][0]         \n",
      "                                                                 block1a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block1a_project_conv (Conv2D)   (None, 16, 16, 16)   512         block1a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block1a_project_bn (BatchNormal (None, 16, 16, 16)   64          block1a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block2a_expand_conv (Conv2D)    (None, 16, 16, 96)   1536        block1a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2a_expand_bn (BatchNormali (None, 16, 16, 96)   384         block2a_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2a_expand_activation (Acti (None, 16, 16, 96)   0           block2a_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2a_dwconv_pad (ZeroPadding (None, 17, 17, 96)   0           block2a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block2a_dwconv (DepthwiseConv2D (None, 8, 8, 96)     864         block2a_dwconv_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2a_bn (BatchNormalization) (None, 8, 8, 96)     384         block2a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block2a_activation (Activation) (None, 8, 8, 96)     0           block2a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block2a_se_squeeze (GlobalAvera (None, 96)           0           block2a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2a_se_reshape (Reshape)    (None, 1, 1, 96)     0           block2a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2a_se_reduce (Conv2D)      (None, 1, 1, 4)      388         block2a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2a_se_expand (Conv2D)      (None, 1, 1, 96)     480         block2a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2a_se_excite (Multiply)    (None, 8, 8, 96)     0           block2a_activation[0][0]         \n",
      "                                                                 block2a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2a_project_conv (Conv2D)   (None, 8, 8, 24)     2304        block2a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2a_project_bn (BatchNormal (None, 8, 8, 24)     96          block2a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block2b_expand_conv (Conv2D)    (None, 8, 8, 144)    3456        block2a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2b_expand_bn (BatchNormali (None, 8, 8, 144)    576         block2b_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2b_expand_activation (Acti (None, 8, 8, 144)    0           block2b_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2b_dwconv (DepthwiseConv2D (None, 8, 8, 144)    1296        block2b_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block2b_bn (BatchNormalization) (None, 8, 8, 144)    576         block2b_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block2b_activation (Activation) (None, 8, 8, 144)    0           block2b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block2b_se_squeeze (GlobalAvera (None, 144)          0           block2b_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2b_se_reshape (Reshape)    (None, 1, 1, 144)    0           block2b_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2b_se_reduce (Conv2D)      (None, 1, 1, 6)      870         block2b_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2b_se_expand (Conv2D)      (None, 1, 1, 144)    1008        block2b_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2b_se_excite (Multiply)    (None, 8, 8, 144)    0           block2b_activation[0][0]         \n",
      "                                                                 block2b_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2b_project_conv (Conv2D)   (None, 8, 8, 24)     3456        block2b_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block2b_project_bn (BatchNormal (None, 8, 8, 24)     96          block2b_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block2b_drop (Dropout)          (None, 8, 8, 24)     0           block2b_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2b_add (Add)               (None, 8, 8, 24)     0           block2b_drop[0][0]               \n",
      "                                                                 block2a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3a_expand_conv (Conv2D)    (None, 8, 8, 144)    3456        block2b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3a_expand_bn (BatchNormali (None, 8, 8, 144)    576         block3a_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3a_expand_activation (Acti (None, 8, 8, 144)    0           block3a_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3a_dwconv_pad (ZeroPadding (None, 11, 11, 144)  0           block3a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block3a_dwconv (DepthwiseConv2D (None, 4, 4, 144)    3600        block3a_dwconv_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3a_bn (BatchNormalization) (None, 4, 4, 144)    576         block3a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block3a_activation (Activation) (None, 4, 4, 144)    0           block3a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block3a_se_squeeze (GlobalAvera (None, 144)          0           block3a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3a_se_reshape (Reshape)    (None, 1, 1, 144)    0           block3a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3a_se_reduce (Conv2D)      (None, 1, 1, 6)      870         block3a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3a_se_expand (Conv2D)      (None, 1, 1, 144)    1008        block3a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3a_se_excite (Multiply)    (None, 4, 4, 144)    0           block3a_activation[0][0]         \n",
      "                                                                 block3a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3a_project_conv (Conv2D)   (None, 4, 4, 40)     5760        block3a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3a_project_bn (BatchNormal (None, 4, 4, 40)     160         block3a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block3b_expand_conv (Conv2D)    (None, 4, 4, 240)    9600        block3a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3b_expand_bn (BatchNormali (None, 4, 4, 240)    960         block3b_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3b_expand_activation (Acti (None, 4, 4, 240)    0           block3b_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3b_dwconv (DepthwiseConv2D (None, 4, 4, 240)    6000        block3b_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block3b_bn (BatchNormalization) (None, 4, 4, 240)    960         block3b_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block3b_activation (Activation) (None, 4, 4, 240)    0           block3b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block3b_se_squeeze (GlobalAvera (None, 240)          0           block3b_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3b_se_reshape (Reshape)    (None, 1, 1, 240)    0           block3b_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3b_se_reduce (Conv2D)      (None, 1, 1, 10)     2410        block3b_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3b_se_expand (Conv2D)      (None, 1, 1, 240)    2640        block3b_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3b_se_excite (Multiply)    (None, 4, 4, 240)    0           block3b_activation[0][0]         \n",
      "                                                                 block3b_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3b_project_conv (Conv2D)   (None, 4, 4, 40)     9600        block3b_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block3b_project_bn (BatchNormal (None, 4, 4, 40)     160         block3b_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block3b_drop (Dropout)          (None, 4, 4, 40)     0           block3b_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3b_add (Add)               (None, 4, 4, 40)     0           block3b_drop[0][0]               \n",
      "                                                                 block3a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4a_expand_conv (Conv2D)    (None, 4, 4, 240)    9600        block3b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4a_expand_bn (BatchNormali (None, 4, 4, 240)    960         block4a_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4a_expand_activation (Acti (None, 4, 4, 240)    0           block4a_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4a_dwconv_pad (ZeroPadding (None, 5, 5, 240)    0           block4a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block4a_dwconv (DepthwiseConv2D (None, 2, 2, 240)    2160        block4a_dwconv_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4a_bn (BatchNormalization) (None, 2, 2, 240)    960         block4a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block4a_activation (Activation) (None, 2, 2, 240)    0           block4a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block4a_se_squeeze (GlobalAvera (None, 240)          0           block4a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4a_se_reshape (Reshape)    (None, 1, 1, 240)    0           block4a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4a_se_reduce (Conv2D)      (None, 1, 1, 10)     2410        block4a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4a_se_expand (Conv2D)      (None, 1, 1, 240)    2640        block4a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4a_se_excite (Multiply)    (None, 2, 2, 240)    0           block4a_activation[0][0]         \n",
      "                                                                 block4a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4a_project_conv (Conv2D)   (None, 2, 2, 80)     19200       block4a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4a_project_bn (BatchNormal (None, 2, 2, 80)     320         block4a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block4b_expand_conv (Conv2D)    (None, 2, 2, 480)    38400       block4a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4b_expand_bn (BatchNormali (None, 2, 2, 480)    1920        block4b_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4b_expand_activation (Acti (None, 2, 2, 480)    0           block4b_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4b_dwconv (DepthwiseConv2D (None, 2, 2, 480)    4320        block4b_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block4b_bn (BatchNormalization) (None, 2, 2, 480)    1920        block4b_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block4b_activation (Activation) (None, 2, 2, 480)    0           block4b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block4b_se_squeeze (GlobalAvera (None, 480)          0           block4b_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4b_se_reshape (Reshape)    (None, 1, 1, 480)    0           block4b_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4b_se_reduce (Conv2D)      (None, 1, 1, 20)     9620        block4b_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4b_se_expand (Conv2D)      (None, 1, 1, 480)    10080       block4b_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4b_se_excite (Multiply)    (None, 2, 2, 480)    0           block4b_activation[0][0]         \n",
      "                                                                 block4b_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4b_project_conv (Conv2D)   (None, 2, 2, 80)     38400       block4b_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4b_project_bn (BatchNormal (None, 2, 2, 80)     320         block4b_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block4b_drop (Dropout)          (None, 2, 2, 80)     0           block4b_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4b_add (Add)               (None, 2, 2, 80)     0           block4b_drop[0][0]               \n",
      "                                                                 block4a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4c_expand_conv (Conv2D)    (None, 2, 2, 480)    38400       block4b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4c_expand_bn (BatchNormali (None, 2, 2, 480)    1920        block4c_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4c_expand_activation (Acti (None, 2, 2, 480)    0           block4c_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4c_dwconv (DepthwiseConv2D (None, 2, 2, 480)    4320        block4c_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block4c_bn (BatchNormalization) (None, 2, 2, 480)    1920        block4c_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block4c_activation (Activation) (None, 2, 2, 480)    0           block4c_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block4c_se_squeeze (GlobalAvera (None, 480)          0           block4c_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4c_se_reshape (Reshape)    (None, 1, 1, 480)    0           block4c_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4c_se_reduce (Conv2D)      (None, 1, 1, 20)     9620        block4c_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4c_se_expand (Conv2D)      (None, 1, 1, 480)    10080       block4c_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4c_se_excite (Multiply)    (None, 2, 2, 480)    0           block4c_activation[0][0]         \n",
      "                                                                 block4c_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4c_project_conv (Conv2D)   (None, 2, 2, 80)     38400       block4c_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block4c_project_bn (BatchNormal (None, 2, 2, 80)     320         block4c_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block4c_drop (Dropout)          (None, 2, 2, 80)     0           block4c_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4c_add (Add)               (None, 2, 2, 80)     0           block4c_drop[0][0]               \n",
      "                                                                 block4b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5a_expand_conv (Conv2D)    (None, 2, 2, 480)    38400       block4c_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5a_expand_bn (BatchNormali (None, 2, 2, 480)    1920        block5a_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5a_expand_activation (Acti (None, 2, 2, 480)    0           block5a_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5a_dwconv (DepthwiseConv2D (None, 2, 2, 480)    12000       block5a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block5a_bn (BatchNormalization) (None, 2, 2, 480)    1920        block5a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block5a_activation (Activation) (None, 2, 2, 480)    0           block5a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block5a_se_squeeze (GlobalAvera (None, 480)          0           block5a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5a_se_reshape (Reshape)    (None, 1, 1, 480)    0           block5a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5a_se_reduce (Conv2D)      (None, 1, 1, 20)     9620        block5a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5a_se_expand (Conv2D)      (None, 1, 1, 480)    10080       block5a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5a_se_excite (Multiply)    (None, 2, 2, 480)    0           block5a_activation[0][0]         \n",
      "                                                                 block5a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5a_project_conv (Conv2D)   (None, 2, 2, 112)    53760       block5a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5a_project_bn (BatchNormal (None, 2, 2, 112)    448         block5a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block5b_expand_conv (Conv2D)    (None, 2, 2, 672)    75264       block5a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5b_expand_bn (BatchNormali (None, 2, 2, 672)    2688        block5b_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5b_expand_activation (Acti (None, 2, 2, 672)    0           block5b_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5b_dwconv (DepthwiseConv2D (None, 2, 2, 672)    16800       block5b_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block5b_bn (BatchNormalization) (None, 2, 2, 672)    2688        block5b_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block5b_activation (Activation) (None, 2, 2, 672)    0           block5b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block5b_se_squeeze (GlobalAvera (None, 672)          0           block5b_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5b_se_reshape (Reshape)    (None, 1, 1, 672)    0           block5b_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5b_se_reduce (Conv2D)      (None, 1, 1, 28)     18844       block5b_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5b_se_expand (Conv2D)      (None, 1, 1, 672)    19488       block5b_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5b_se_excite (Multiply)    (None, 2, 2, 672)    0           block5b_activation[0][0]         \n",
      "                                                                 block5b_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5b_project_conv (Conv2D)   (None, 2, 2, 112)    75264       block5b_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5b_project_bn (BatchNormal (None, 2, 2, 112)    448         block5b_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block5b_drop (Dropout)          (None, 2, 2, 112)    0           block5b_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5b_add (Add)               (None, 2, 2, 112)    0           block5b_drop[0][0]               \n",
      "                                                                 block5a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5c_expand_conv (Conv2D)    (None, 2, 2, 672)    75264       block5b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5c_expand_bn (BatchNormali (None, 2, 2, 672)    2688        block5c_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5c_expand_activation (Acti (None, 2, 2, 672)    0           block5c_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5c_dwconv (DepthwiseConv2D (None, 2, 2, 672)    16800       block5c_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block5c_bn (BatchNormalization) (None, 2, 2, 672)    2688        block5c_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block5c_activation (Activation) (None, 2, 2, 672)    0           block5c_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block5c_se_squeeze (GlobalAvera (None, 672)          0           block5c_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5c_se_reshape (Reshape)    (None, 1, 1, 672)    0           block5c_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5c_se_reduce (Conv2D)      (None, 1, 1, 28)     18844       block5c_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5c_se_expand (Conv2D)      (None, 1, 1, 672)    19488       block5c_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5c_se_excite (Multiply)    (None, 2, 2, 672)    0           block5c_activation[0][0]         \n",
      "                                                                 block5c_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5c_project_conv (Conv2D)   (None, 2, 2, 112)    75264       block5c_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block5c_project_bn (BatchNormal (None, 2, 2, 112)    448         block5c_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block5c_drop (Dropout)          (None, 2, 2, 112)    0           block5c_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5c_add (Add)               (None, 2, 2, 112)    0           block5c_drop[0][0]               \n",
      "                                                                 block5b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block6a_expand_conv (Conv2D)    (None, 2, 2, 672)    75264       block5c_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block6a_expand_bn (BatchNormali (None, 2, 2, 672)    2688        block6a_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6a_expand_activation (Acti (None, 2, 2, 672)    0           block6a_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6a_dwconv_pad (ZeroPadding (None, 5, 5, 672)    0           block6a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block6a_dwconv (DepthwiseConv2D (None, 1, 1, 672)    16800       block6a_dwconv_pad[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6a_bn (BatchNormalization) (None, 1, 1, 672)    2688        block6a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block6a_activation (Activation) (None, 1, 1, 672)    0           block6a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block6a_se_squeeze (GlobalAvera (None, 672)          0           block6a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6a_se_reshape (Reshape)    (None, 1, 1, 672)    0           block6a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6a_se_reduce (Conv2D)      (None, 1, 1, 28)     18844       block6a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6a_se_expand (Conv2D)      (None, 1, 1, 672)    19488       block6a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6a_se_excite (Multiply)    (None, 1, 1, 672)    0           block6a_activation[0][0]         \n",
      "                                                                 block6a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6a_project_conv (Conv2D)   (None, 1, 1, 192)    129024      block6a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6a_project_bn (BatchNormal (None, 1, 1, 192)    768         block6a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block6b_expand_conv (Conv2D)    (None, 1, 1, 1152)   221184      block6a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6b_expand_bn (BatchNormali (None, 1, 1, 1152)   4608        block6b_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6b_expand_activation (Acti (None, 1, 1, 1152)   0           block6b_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6b_dwconv (DepthwiseConv2D (None, 1, 1, 1152)   28800       block6b_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block6b_bn (BatchNormalization) (None, 1, 1, 1152)   4608        block6b_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block6b_activation (Activation) (None, 1, 1, 1152)   0           block6b_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block6b_se_squeeze (GlobalAvera (None, 1152)         0           block6b_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6b_se_reshape (Reshape)    (None, 1, 1, 1152)   0           block6b_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6b_se_reduce (Conv2D)      (None, 1, 1, 48)     55344       block6b_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6b_se_expand (Conv2D)      (None, 1, 1, 1152)   56448       block6b_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6b_se_excite (Multiply)    (None, 1, 1, 1152)   0           block6b_activation[0][0]         \n",
      "                                                                 block6b_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6b_project_conv (Conv2D)   (None, 1, 1, 192)    221184      block6b_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6b_project_bn (BatchNormal (None, 1, 1, 192)    768         block6b_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block6b_drop (Dropout)          (None, 1, 1, 192)    0           block6b_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6b_add (Add)               (None, 1, 1, 192)    0           block6b_drop[0][0]               \n",
      "                                                                 block6a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6c_expand_conv (Conv2D)    (None, 1, 1, 1152)   221184      block6b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block6c_expand_bn (BatchNormali (None, 1, 1, 1152)   4608        block6c_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6c_expand_activation (Acti (None, 1, 1, 1152)   0           block6c_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6c_dwconv (DepthwiseConv2D (None, 1, 1, 1152)   28800       block6c_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block6c_bn (BatchNormalization) (None, 1, 1, 1152)   4608        block6c_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block6c_activation (Activation) (None, 1, 1, 1152)   0           block6c_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block6c_se_squeeze (GlobalAvera (None, 1152)         0           block6c_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6c_se_reshape (Reshape)    (None, 1, 1, 1152)   0           block6c_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6c_se_reduce (Conv2D)      (None, 1, 1, 48)     55344       block6c_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6c_se_expand (Conv2D)      (None, 1, 1, 1152)   56448       block6c_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6c_se_excite (Multiply)    (None, 1, 1, 1152)   0           block6c_activation[0][0]         \n",
      "                                                                 block6c_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6c_project_conv (Conv2D)   (None, 1, 1, 192)    221184      block6c_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6c_project_bn (BatchNormal (None, 1, 1, 192)    768         block6c_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block6c_drop (Dropout)          (None, 1, 1, 192)    0           block6c_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6c_add (Add)               (None, 1, 1, 192)    0           block6c_drop[0][0]               \n",
      "                                                                 block6b_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block6d_expand_conv (Conv2D)    (None, 1, 1, 1152)   221184      block6c_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block6d_expand_bn (BatchNormali (None, 1, 1, 1152)   4608        block6d_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6d_expand_activation (Acti (None, 1, 1, 1152)   0           block6d_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6d_dwconv (DepthwiseConv2D (None, 1, 1, 1152)   28800       block6d_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block6d_bn (BatchNormalization) (None, 1, 1, 1152)   4608        block6d_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block6d_activation (Activation) (None, 1, 1, 1152)   0           block6d_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block6d_se_squeeze (GlobalAvera (None, 1152)         0           block6d_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6d_se_reshape (Reshape)    (None, 1, 1, 1152)   0           block6d_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6d_se_reduce (Conv2D)      (None, 1, 1, 48)     55344       block6d_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6d_se_expand (Conv2D)      (None, 1, 1, 1152)   56448       block6d_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6d_se_excite (Multiply)    (None, 1, 1, 1152)   0           block6d_activation[0][0]         \n",
      "                                                                 block6d_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6d_project_conv (Conv2D)   (None, 1, 1, 192)    221184      block6d_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block6d_project_bn (BatchNormal (None, 1, 1, 192)    768         block6d_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block6d_drop (Dropout)          (None, 1, 1, 192)    0           block6d_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6d_add (Add)               (None, 1, 1, 192)    0           block6d_drop[0][0]               \n",
      "                                                                 block6c_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block7a_expand_conv (Conv2D)    (None, 1, 1, 1152)   221184      block6d_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block7a_expand_bn (BatchNormali (None, 1, 1, 1152)   4608        block7a_expand_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7a_expand_activation (Acti (None, 1, 1, 1152)   0           block7a_expand_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block7a_dwconv (DepthwiseConv2D (None, 1, 1, 1152)   10368       block7a_expand_activation[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "block7a_bn (BatchNormalization) (None, 1, 1, 1152)   4608        block7a_dwconv[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block7a_activation (Activation) (None, 1, 1, 1152)   0           block7a_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "block7a_se_squeeze (GlobalAvera (None, 1152)         0           block7a_activation[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7a_se_reshape (Reshape)    (None, 1, 1, 1152)   0           block7a_se_squeeze[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7a_se_reduce (Conv2D)      (None, 1, 1, 48)     55344       block7a_se_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7a_se_expand (Conv2D)      (None, 1, 1, 1152)   56448       block7a_se_reduce[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block7a_se_excite (Multiply)    (None, 1, 1, 1152)   0           block7a_activation[0][0]         \n",
      "                                                                 block7a_se_expand[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block7a_project_conv (Conv2D)   (None, 1, 1, 320)    368640      block7a_se_excite[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block7a_project_bn (BatchNormal (None, 1, 1, 320)    1280        block7a_project_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "top_conv (Conv2D)               (None, 1, 1, 1280)   409600      block7a_project_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "top_bn (BatchNormalization)     (None, 1, 1, 1280)   5120        top_conv[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "top_activation (Activation)     (None, 1, 1, 1280)   0           top_bn[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,049,571\n",
      "Trainable params: 4,007,548\n",
      "Non-trainable params: 42,023\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "efficient = EfficientNetB0(  #weights = 'imagenet',  #그냥 초기화 하는거면, 이거 지우기\n",
    "                            include_top = False,\n",
    "                            input_shape=(32,32,3)\n",
    "                            )\n",
    "\n",
    "# vgg conv 구조만 사용하고 마지막 FC layer는 다른거 사용\n",
    "\n",
    "fc_layer = keras.Sequential([\n",
    "                             layers.Flatten(),\n",
    "                             layers.Dense(1024, activation = 'relu'),\n",
    "                             layers.Dense(1024, activation = 'relu'),\n",
    "                             layers.Dense(1024, activation = 'relu'),\n",
    "                             layers.Dense(10, activation = \"sigmoid\")\n",
    "                             ])\n",
    "\n",
    "model = keras.Sequential([efficient,\n",
    "                          fc_layer\n",
    "                          ])\n",
    "model.summary()\n",
    "# efficient.summary()\n",
    "# fc_layer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VuUFfKa1OIUa"
   },
   "outputs": [],
   "source": [
    "# Callback 함수 지정 해주기\n",
    "early_stop = EarlyStopping(patience=30) \n",
    "mc = ModelCheckpoint(\"./best_model/efficient_original_checkpoint\", \n",
    "                     save_best_only=True,\n",
    "                     monitor = 'val_loss',\n",
    "                     verbose = 1,\n",
    "                     mode = 'min') \n",
    "reduce_lr  = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                               factor=0.5, \n",
    "                               patience=5\n",
    "                               ) \n",
    "# tensorboard 관련 \n",
    "def make_Tensorboard_dir(dir_name):\n",
    "  root_logdir = os.path.join(os.curdir, dir_name)\n",
    "  sub_dir_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  return os.path.join(root_logdir, sub_dir_name)\n",
    "\n",
    "dir_name = \"Learning_log\"\n",
    "TB_log_dir = make_Tensorboard_dir(dir_name)\n",
    "TensorB = TensorBoard(log_dir = TB_log_dir)\n",
    "\n",
    "\n",
    "#optimizaer  조정 해주기\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3K9FF5gmOLH6",
    "outputId": "2f7c3f80-6d86-4397-f4c1-d892168f980c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 178s 117ms/step - loss: 1.6143 - accuracy: 0.4137 - val_loss: 2.6838 - val_accuracy: 0.1906\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.68381, saving model to ./best_model\\efficient_original_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./best_model\\efficient_original_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 155s 110ms/step - loss: 1.2311 - accuracy: 0.5645 - val_loss: 2.3840 - val_accuracy: 0.2124\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.68381 to 2.38399, saving model to ./best_model\\efficient_original_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./best_model\\efficient_original_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 156s 111ms/step - loss: 1.0538 - accuracy: 0.6262 - val_loss: 2.8173 - val_accuracy: 0.2250\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.38399\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 155s 110ms/step - loss: 0.9419 - accuracy: 0.6712 - val_loss: 2.2796 - val_accuracy: 0.2872\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.38399 to 2.27961, saving model to ./best_model\\efficient_original_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./best_model\\efficient_original_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 156s 111ms/step - loss: 0.8416 - accuracy: 0.7045 - val_loss: 1.9051 - val_accuracy: 0.4004\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.27961 to 1.90506, saving model to ./best_model\\efficient_original_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./best_model\\efficient_original_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 157s 111ms/step - loss: 0.7563 - accuracy: 0.7324 - val_loss: 2.1938 - val_accuracy: 0.2806\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.90506\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 157s 111ms/step - loss: 0.6947 - accuracy: 0.7550 - val_loss: 2.4868 - val_accuracy: 0.2830\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.90506\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 160s 114ms/step - loss: 0.6342 - accuracy: 0.7756 - val_loss: 1.4242 - val_accuracy: 0.5380\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.90506 to 1.42415, saving model to ./best_model\\efficient_original_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./best_model\\efficient_original_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 151s 108ms/step - loss: 0.5741 - accuracy: 0.7963 - val_loss: 3.0591 - val_accuracy: 0.2464\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.42415\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 152s 108ms/step - loss: 0.5290 - accuracy: 0.8112 - val_loss: 1.3816 - val_accuracy: 0.5446\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.42415 to 1.38161, saving model to ./best_model\\efficient_original_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./best_model\\efficient_original_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 225s 160ms/step - loss: 0.4856 - accuracy: 0.8288 - val_loss: 4.2319 - val_accuracy: 0.2302\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.38161\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 143s 102ms/step - loss: 0.4359 - accuracy: 0.8441 - val_loss: 2.0350 - val_accuracy: 0.4178\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.38161\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 144s 103ms/step - loss: 0.4120 - accuracy: 0.8538 - val_loss: 1.9505 - val_accuracy: 0.4674\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.38161\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 146s 104ms/step - loss: 0.3769 - accuracy: 0.8667 - val_loss: 2.5804 - val_accuracy: 0.3154\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.38161\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 146s 104ms/step - loss: 0.3511 - accuracy: 0.8732 - val_loss: 2.0642 - val_accuracy: 0.4724\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.38161\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 145s 103ms/step - loss: 0.2802 - accuracy: 0.8996 - val_loss: 2.4960 - val_accuracy: 0.5010acy: - ETA: 14s - loss: 0.28 - ETA: 6s - loss: 0.2801  - E - ETA: 3s - loss: 0.2801 -  - ETA: 3s - loss: 0.2803 \n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.38161\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 147s 105ms/step - loss: 0.2496 - accuracy: 0.9104 - val_loss: 1.9023 - val_accuracy: 0.5444\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.38161\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 133s 94ms/step - loss: 0.2300 - accuracy: 0.9185 - val_loss: 2.3092 - val_accuracy: 0.5024\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.38161\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 134s 95ms/step - loss: 0.2153 - accuracy: 0.9236 - val_loss: 0.9156 - val_accuracy: 0.7486\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.38161 to 0.91557, saving model to ./best_model\\efficient_original_checkpoint\n",
      "INFO:tensorflow:Assets written to: ./best_model\\efficient_original_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 152s 108ms/step - loss: 0.2017 - accuracy: 0.9288 - val_loss: 2.3075 - val_accuracy: 0.4862\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.91557\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 133s 94ms/step - loss: 0.1949 - accuracy: 0.9308 - val_loss: 1.4020 - val_accuracy: 0.6540\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.91557\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 132s 94ms/step - loss: 0.1793 - accuracy: 0.9362 - val_loss: 1.6318 - val_accuracy: 0.6288\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.91557\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 132s 94ms/step - loss: 0.1735 - accuracy: 0.9395 - val_loss: 2.3872 - val_accuracy: 0.4710\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.91557\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 131s 93ms/step - loss: 0.1649 - accuracy: 0.9422 - val_loss: 1.2391 - val_accuracy: 0.7164\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.91557\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 131s 93ms/step - loss: 0.1391 - accuracy: 0.9518 - val_loss: 1.4762 - val_accuracy: 0.6788\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.91557\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 132s 94ms/step - loss: 0.1287 - accuracy: 0.9556 - val_loss: 1.2744 - val_accuracy: 0.7046\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.91557\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 132s 94ms/step - loss: 0.1200 - accuracy: 0.9578 - val_loss: 1.1471 - val_accuracy: 0.7386\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.91557\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 132s 94ms/step - loss: 0.1184 - accuracy: 0.9596 - val_loss: 1.1400 - val_accuracy: 0.7440\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.91557\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 132s 94ms/step - loss: 0.1112 - accuracy: 0.9611 - val_loss: 1.1898 - val_accuracy: 0.7368\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.91557\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 134s 95ms/step - loss: 0.1010 - accuracy: 0.9650 - val_loss: 1.0225 - val_accuracy: 0.7692\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.91557\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 133s 94ms/step - loss: 0.0995 - accuracy: 0.9659 - val_loss: 1.0125 - val_accuracy: 0.77540.0995 \n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.91557\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 133s 94ms/step - loss: 0.0944 - accuracy: 0.9666 - val_loss: 1.0137 - val_accuracy: 0.7632\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.91557\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 133s 94ms/step - loss: 0.0924 - accuracy: 0.9680 - val_loss: 1.0537 - val_accuracy: 0.7694\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.91557\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 132s 94ms/step - loss: 0.0914 - accuracy: 0.9683 - val_loss: 1.0263 - val_accuracy: 0.7698\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.91557\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 133s 95ms/step - loss: 0.0875 - accuracy: 0.9700 - val_loss: 1.0050 - val_accuracy: 0.7784\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.91557\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 132s 94ms/step - loss: 0.0834 - accuracy: 0.9714 - val_loss: 1.0098 - val_accuracy: 0.7774\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.91557\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 134s 95ms/step - loss: 0.0827 - accuracy: 0.9722 - val_loss: 1.0819 - val_accuracy: 0.7624\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.91557\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 133s 95ms/step - loss: 0.0829 - accuracy: 0.9712 - val_loss: 1.0232 - val_accuracy: 0.7800\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.91557\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 134s 95ms/step - loss: 0.0807 - accuracy: 0.9726 - val_loss: 1.0121 - val_accuracy: 0.7796\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.91557\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 133s 95ms/step - loss: 0.0786 - accuracy: 0.9740 - val_loss: 1.0070 - val_accuracy: 0.7850\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.91557\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 133s 95ms/step - loss: 0.0772 - accuracy: 0.9740 - val_loss: 1.0072 - val_accuracy: 0.7822\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.91557\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 133s 94ms/step - loss: 0.0783 - accuracy: 0.9736 - val_loss: 1.0267 - val_accuracy: 0.7796\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.91557\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 132s 94ms/step - loss: 0.0770 - accuracy: 0.9747 - val_loss: 1.0163 - val_accuracy: 0.7818\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.91557\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 133s 95ms/step - loss: 0.0785 - accuracy: 0.9735 - val_loss: 1.0112 - val_accuracy: 0.7816\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.91557\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 132s 94ms/step - loss: 0.0758 - accuracy: 0.9739 - val_loss: 1.0165 - val_accuracy: 0.7846\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.91557\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 132s 94ms/step - loss: 0.0733 - accuracy: 0.9751 - val_loss: 1.0086 - val_accuracy: 0.7820\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.91557\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 133s 94ms/step - loss: 0.0742 - accuracy: 0.9741 - val_loss: 1.0160 - val_accuracy: 0.7840- ETA: 1s - loss: 0.0742 - accuracy: 0. - ETA\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.91557\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 131s 93ms/step - loss: 0.0745 - accuracy: 0.9745 - val_loss: 1.0176 - val_accuracy: 0.7804\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.91557\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 131s 93ms/step - loss: 0.0727 - accuracy: 0.9753 - val_loss: 1.0185 - val_accuracy: 0.7834\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.91557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23cb3927ac8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer, loss 함수를 정의하고,  학습 준비를 한다,  metrics 는 어떤 일이 발생하는지 보여줄 것들\n",
    "model.compile(optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "# 한번에 몇개의 데이터 학습하고 가중치 갱신할지 \n",
    "model.fit(train_x, train_y,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          batch_size=32,\n",
    "          #validation_split = 0.1\n",
    "          validation_data = (valid_x, valid_y),\n",
    "          callbacks = [early_stop, reduce_lr , mc]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rxjhscjOR5w",
    "outputId": "597bc3a8-b80a-4e88-de02-dbda97b583bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 29ms/step - loss: 1.0514 - accuracy: 0.7750\n",
      "loss= 1.051389217376709\n",
      "acc= 0.7749999761581421\n",
      "[5 8 8 ... 5 0 7]\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_x, test_y)\n",
    "print(\"loss=\",loss)\n",
    "print(\"acc=\",acc)\n",
    "\n",
    "y_ = model.predict(test_x)\n",
    "predicted = np.argmax(y_, axis=1)\n",
    "\n",
    "print(predicted)\n",
    "\n",
    "## 75.2\n",
    "## 67.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8AuRmRIqOTXd",
    "outputId": "b1baed42-f47a-4924-ffb7-17d4988046a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/efficient_original_checkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./saved_model/efficient_original.h5\")\n",
    "model.save(\"./saved_model/efficient_original_checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMQkmsTBOdJC"
   },
   "source": [
    "##int_8 Quantization 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8mdt9FmAOUj9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_447247) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_446687) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_414219) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_442124) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_414618) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_450257) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_450490) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_415000) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_451035) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_444455) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_414827) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_416629) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_415216) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_416321) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_450864) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_416853) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_443895) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_416985) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_417418) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_445706) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_416420) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_442513) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_415873) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_451097) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_442706) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_417077) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_446920) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_416812) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_414003) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_447480) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_445473) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_416097) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_448654) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_445644) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_445084) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_449883) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_445146) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_417260) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference__wrapped_model_406610) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_444891) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_444284) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_449214) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_443266) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_414868) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_414120) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_415664) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_1_layer_call_and_return_conditional_losses_435749) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_442186) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_449650) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_414560) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_450428) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_416588) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_415965) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_415308) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_444517) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_442768) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_415756) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_415267) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_415715) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_443957) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_446858) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_415924) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_415491) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_449043) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_443328) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_414659) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_416189) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_448461) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_413911) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_446080) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_414776) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_414387) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_448087) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_438653) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_447418) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_440406) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_443095) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_416761) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_417036) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_415440) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_446251) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_416537) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_449276) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_sequential_1_layer_call_and_return_conditional_losses_433966) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_415532) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_415058) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_448025) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_414336) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_447854) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_414178) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_451424) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_416379) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_449821) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_441953) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_443702) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_417209) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_446313) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_448716) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_416148) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_414428) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_415099) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_417301) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:absl:Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_413962) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    }
   ],
   "source": [
    "# 구조까지 들어가 있는거\n",
    "model = tf.keras.models.load_model('./best_model/efficient_original_checkpoint')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mnkSbZd_Oc3R"
   },
   "outputs": [],
   "source": [
    "# input 에 대해서 변수 설정\n",
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(train_x).batch(1).take(100):\n",
    "    yield [input_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5NJDbarOlhc",
    "outputId": "335fc300-1d60-42ff-9b1c-eb01e77fd395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\yutan\\AppData\\Local\\Temp\\tmpg0mmey_o\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\yutan\\AppData\\Local\\Temp\\tmpg0mmey_o\\assets\n",
      "C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n",
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    }
   ],
   "source": [
    "# int 8 로 quantization 진행하기\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qIfHHR7QOm60",
    "outputId": "beae9200-3143-4ffb-b74e-86a7405cc20c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\yutan\\AppData\\Local\\Temp\\tmpak7zx1cs\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\yutan\\AppData\\Local\\Temp\\tmpak7zx1cs\\assets\n",
      "C:\\Users\\yutan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "# 그냥 파일 형태만 tflite로 변환 (float 형태임)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ncXY1MmKOobq",
    "outputId": "a4cd7d6d-68ed-4e39-9516-c61287abe357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZXPnySKOo6w",
    "outputId": "5248bea5-b071-4771-8ed6-47a87d7fb765"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8353208"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 저장 해주는 과정\n",
    "import pathlib\n",
    "\n",
    "tflite_models_dir = pathlib.Path(\"./tflite/efficient_tflite/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the unquantized/float model:\n",
    "tflite_model_file = tflite_models_dir/\"efficient_original_tflite.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)\n",
    "# Save the quantized model:\n",
    "tflite_model_quant_file = tflite_models_dir/\"efficient_quantization_tflite.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Su6SO1CKOqLi"
   },
   "outputs": [],
   "source": [
    "def run_tflite_model(tflite_file, test_image_indices):\n",
    "  global test_x\n",
    "\n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
    "  for i, test_image_index in enumerate(test_image_indices):\n",
    "    test_image = test_x[test_image_index]\n",
    "    test_label = test_y[test_image_index]\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to uint8\n",
    "    if input_details['dtype'] == np.uint8:\n",
    "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "      test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LqT2rNGPOrh5"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(tflite_file, model_type):\n",
    "  global test_x\n",
    "  global test_y\n",
    "\n",
    "  test_image_indices = range(test_x.shape[0])\n",
    "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
    "\n",
    "  accuracy = (np.sum(test_y.reshape(-1)== predictions) * 100) / len(test_x)\n",
    "\n",
    "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
    "      model_type, accuracy, len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-CPgDYzNWcnM",
    "outputId": "58fb64a3-7b2c-4c65-ab57-80db86dae74a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model accuracy is 74.2800% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_model_file, model_type=\"Float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model accuracy is 22.0200% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_model_quant_file, model_type=\"Quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SYzMVTwNOsi7"
   },
   "outputs": [],
   "source": [
    "def run_tflite_time(tflite_file, test_image_indices):\n",
    "  global test_x\n",
    "\n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
    "  for i, test_image_index in enumerate(test_image_indices):\n",
    "    test_image = test_x[test_image_index]\n",
    "    test_label = test_y[test_image_index]\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to uint8\n",
    "    if input_details['dtype'] == np.uint8:\n",
    "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "      test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "        \n",
    "    start = time.time()\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "    end = time.time()\n",
    "    \n",
    "    fps = 1 / (end - start)\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "  return fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to test a different image\n",
    "test_image_index = 1\n",
    "\n",
    "def test_model_time(tflite_file, test_image_index, model_type):\n",
    "  global test_y\n",
    "  fps = run_tflite_time(tflite_file, [test_image_index])\n",
    "  print(fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3_wYKm4jOtj4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.878428395178894\n"
     ]
    }
   ],
   "source": [
    "test_model_time(tflite_model_file, test_image_index, model_type=\"Float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Ti97vld-Oume"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.684436936854409\n"
     ]
    }
   ],
   "source": [
    "test_model_time(tflite_model_quant_file, test_image_index, model_type=\"Quantized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q85dwKpQOwv1"
   },
   "source": [
    "##Pruning 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9SM32mkOvoT"
   },
   "outputs": [],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    test_x, test_y, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "\n",
    "_, keras_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
    "print('Saved baseline model to:', keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubhlv_QcOzEJ"
   },
   "outputs": [],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Dense, tfmot.sparsity.keras.PrunableLayer):\n",
    "\n",
    "  def get_prunable_weights(self):\n",
    "    # Prune bias also, though that usually harms model accuracy too much.\n",
    "    return [self.kernel, self.bias]\n",
    "\n",
    "# Use `prune_low_magnitude` to make the `MyDenseLayer` layer train with pruning.\n",
    "model_for_pruning = tf.keras.Sequential([\n",
    "  tfmot.sparsity.keras.prune_low_magnitude(MyDenseLayer(20, input_shape=input_shape)),\n",
    "  tf.keras.layers.Flatten()\n",
    "])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xh6Y4z3sO0AI"
   },
   "outputs": [],
   "source": [
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_images = train_x.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGqo2IrqO0__"
   },
   "outputs": [],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "\n",
    "model_for_pruning.fit(train_x, train_,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1PONfqRO165"
   },
   "outputs": [],
   "source": [
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   test_x, test_y, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy) \n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ur8LuxUO3ht"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir={logdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ke3yltbMO4hd"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1.\t경량 딥 러닝 기술 배경\n",
    "  1.\t기술 배경\n",
    "  2.\t기술 동향\n",
    "2.\t경량 알고리즘\n",
    "  1.\t모델 구조\n",
    "    1.\tResidual Block (Resnet)\n",
    "    2.\tDensely Connected Convolutional Networks (Densenet)\n",
    "    3.\tFire module (Squeezenet)\n",
    "  2.\t합성곱 필터 \n",
    "    1.\tDepthwise Convolution (mobilenet)\n",
    "    2.\tPointwise Convolution\n",
    "    3.\tDepthwise Separable Convolution\n",
    "  3.\t자동 모델 탐색\n",
    "    1.\tNetAdapt\n",
    "    2.\tMNasNet\n",
    "4.\t알고리즘 경량화\n",
    "  5.\t모델 압축\n",
    "    1.\tWeight Pruning\n",
    "    2.\tQuantization & Binarization \n",
    "  6.\t지식 증류\n",
    "    1.\t전문가 모델 – 숙련가 모델 \n",
    "  7.\t하드웨어 가속화\n",
    "    1.\tTPU\n",
    "    2.\tVPU\n",
    "    3.\t젝슨 TX2\n",
    "    4.\t퀄컴 스냅드래곤\n",
    "    5.\tA12 칩\n",
    "    6.\t엑시노스\n",
    "    7.\t등등 NPU \n",
    "  8.\t모델 압축 자동 탐색\n",
    "    1.\tTencent의 PocketFlow\n",
    "    2.\tAutoMC (Automated Model Compression)\t\n",
    "  9.\t실험 \n",
    "    1.\t비교 모델\n",
    "    2.\t개발 환경 및 code 구성\n",
    "    3 .\t성능 비교\n",
    "    4.\tInsight\n",
    "10.\t차주 계획\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNmT48vALjG+hejQmPj4Wuo",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MobileNet_V3_Small_예시.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

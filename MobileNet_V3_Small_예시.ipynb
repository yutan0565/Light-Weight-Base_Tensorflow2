{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNet_V3_Small_예시.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Et-iwTi6UUUc5F1_JvqO-_LjfL43QBV_",
      "authorship_tag": "ABX9TyMlGpti5DnzUXjBM+ubcyhp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yutan0565/colab_git/blob/main/MobileNet_V3_Small_%EC%98%88%EC%8B%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 기본 모델 형성\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2HgXJWEEeK0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pathlib\n",
        "\n",
        "from tensorflow.keras.applications import MobileNetV3Small\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
        "\n"
      ],
      "metadata": {
        "id": "fO2RGXR2JRxq"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(raw_train_x, raw_train_y), (raw_test_x, raw_test_y) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "\"\"\"\n",
        "print(raw_train_x.shape)\n",
        "print(raw_test_x.shape)\n",
        "print(raw_train_y.shape)\n",
        "print(raw_test_y.shape)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0ptbBJ19JRz_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "95f0434b-0005-40e4-c448-9f7203f8a8f2"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(raw_train_x.shape)\\nprint(raw_test_x.shape)\\nprint(raw_train_y.shape)\\nprint(raw_test_y.shape)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# float을 넣을거면 0~1 사이 값으로 바꿔야함\n",
        "train_x = raw_train_x[:45000].astype(np.float32)/255.0\n",
        "valid_x = raw_train_x[45000:].astype(np.float32)/255.0\n",
        "test_x = raw_test_x.astype(np.float32)/255.0\n",
        "\n",
        "\n",
        "train_y = raw_train_y[:45000]\n",
        "valid_y = raw_train_y[45000:]\n",
        "test_y = raw_test_y\n",
        "labels = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "print(train_x.shape)\n",
        "print(valid_x.shape)\n",
        "print(test_x.shape)\n",
        "print(train_y.shape)\n",
        "print(valid_y.shape)\n",
        "print(test_y.shape)\n",
        "\n",
        "def show_sample(i):\n",
        "  print(raw_train_y[i][0], labels[raw_train_y[i][0]])\n",
        "  plt.imshow(raw_train_x[i])\n",
        "  plt.show()\n",
        "\n",
        "for i in [2, 10, 12, 14]:\n",
        "  show_sample(i)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Y3XsyJumvC4Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b0fb4928-0132-47bd-e67b-c1df8e680c6e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(train_x.shape)\\nprint(valid_x.shape)\\nprint(test_x.shape)\\nprint(train_y.shape)\\nprint(valid_y.shape)\\nprint(test_y.shape)\\n\\ndef show_sample(i):\\n  print(raw_train_y[i][0], labels[raw_train_y[i][0]])\\n  plt.imshow(raw_train_x[i])\\n  plt.show()\\n\\nfor i in [2, 10, 12, 14]:\\n  show_sample(i)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mobile = MobileNetV3Small(             # weights = 'imagenet',  그냥 초기화 하는거면, 이거 지우기\n",
        "                            include_top = False,\n",
        "                            input_shape=(32,32,3)\n",
        "                            )\n",
        "\n",
        "# vgg conv 구조만 사용하고 마지막 FC layer는 다른거 사용\n",
        "\n",
        "fc_layer = keras.Sequential([\n",
        "                             layers.Flatten(),\n",
        "                             layers.Dense(512, activation = 'relu'),\n",
        "                             layers.Dense(512, activation = 'relu'),\n",
        "                             layers.Dense(10, activation = \"sigmoid\")\n",
        "                             ])\n",
        "\n",
        "model = keras.Sequential([mobile,\n",
        "                          fc_layer\n",
        "                          ])\n",
        "\n",
        "# mobile.summary()\n",
        "# fc_layer.summary()\n"
      ],
      "metadata": {
        "id": "1nLvUwGGvQ8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5832f0ff-dcb0-49aa-c6d7-55333eaf0ea1"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(patience=20) \n",
        "mc = ModelCheckpoint(\"/check_point.h5\", save_best_only=True) \n",
        "reduce_lr  = ReduceLROnPlateau(monitor = 'val_loss',\n",
        "                               factor=0.1, \n",
        "                               patience=5\n",
        "                               ) \n",
        "csvlogger = CSVLogger(\"model_log.log\") \n",
        "\n",
        "\n",
        "# optimizer, loss 함수를 정의하고,  학습 준비를 한다,  metrics 는 어떤 일이 발생하는지 보여줄 것들\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "\n",
        "# 한번에 몇개의 데이터 학습하고 가중치 갱신할지 \n",
        "model.fit(train_x, train_y,\n",
        "          epochs=50,\n",
        "          verbose=1,\n",
        "          batch_size=128,\n",
        "          #validation_split = 0.1\n",
        "          validation_data = (valid_x, valid_y),\n",
        "          callbacks = [es, mc, reduce_lr , csvlogger]\n",
        "          )\n",
        "\n",
        "# call back 참고 : https://deep-deep-deep.tistory.com/1 들어가면 여러 옵션들이 나옴"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ-LukhIvphi",
        "outputId": "7f2abed6-ed22-4a5b-9c5f-cf371f61daf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "216/352 [=================>............] - ETA: 26s - loss: 1.9345 - accuracy: 0.2958"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(test_x, test_y)\n",
        "print(\"loss=\",loss)\n",
        "print(\"acc=\",acc)\n",
        "\n",
        "y_ = model.predict(test_x)\n",
        "predicted = np.argmax(y_, axis=1)\n",
        "\n",
        "print(predicted)"
      ],
      "metadata": {
        "id": "sIGR4cUIFFFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## int_8 Quantization 진행\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dWaTjCHHeZJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 구조까지 들어가 있는거\n",
        "#model_load = tf.keras.models.load_model('saved_model/my_model')"
      ],
      "metadata": {
        "id": "kR1YvGr6TqB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input 에 대해서 변수 설정\n",
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(train_x).batch(1).take(100):\n",
        "    yield [input_value]"
      ],
      "metadata": {
        "id": "K_pCuwiQSAec"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# int 8 로 quantization 진행하기\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "tflite_model_quant = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvxpgAD7SAvS",
        "outputId": "7d981156-bca2-4c3e-e895-3fb58eca8980"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) MobilenetV3small_input with unsupported characters which will be renamed to mobilenetv3small_input in the SavedModel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpxo41f6ew/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpxo41f6ew/assets\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그냥 파일 형태만 tflite로 변환 (float 형태임)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "tflite_model = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLjaFd7wavtS",
        "outputId": "30281f98-b691-4645-df2a-e6b45233b4d5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) MobilenetV3small_input with unsupported characters which will be renamed to mobilenetv3small_input in the SavedModel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpj_9wxcy8/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpj_9wxcy8/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
        "input_type = interpreter.get_input_details()[0]['dtype']\n",
        "print('input: ', input_type)\n",
        "output_type = interpreter.get_output_details()[0]['dtype']\n",
        "print('output: ', output_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xyG2LcUUJT9",
        "outputId": "3994b193-1c71-4508-c425-93c9ce51aa8e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:  <class 'numpy.uint8'>\n",
            "output:  <class 'numpy.uint8'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장 해주는 과정\n",
        "import pathlib\n",
        "\n",
        "tflite_models_dir = pathlib.Path(\"/tmp/mobile_tflite_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Save the unquantized/float model:\n",
        "tflite_model_file = tflite_models_dir/\"mobile_model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)\n",
        "# Save the quantized model:\n",
        "tflite_model_quant_file = tflite_models_dir/\"mobiile_model_quant.tflite\"\n",
        "tflite_model_quant_file.write_bytes(tflite_model_quant)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdR24DrHUfpg",
        "outputId": "3401e241-add5-4f5b-b807-c5e60edab2d3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1795944"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensorlffow lite 모델 실행 해보기"
      ],
      "metadata": {
        "id": "sAK1WC33UfxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tflite_model(tflite_file, test_image_indices):\n",
        "  global test_x\n",
        "\n",
        "  # Initialize the interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
        "  for i, test_image_index in enumerate(test_image_indices):\n",
        "    test_image = test_x[test_image_index]\n",
        "    test_label = test_y[test_image_index]\n",
        "\n",
        "    # Check if the input type is quantized, then rescale input data to uint8\n",
        "    if input_details['dtype'] == np.uint8:\n",
        "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "      test_image = test_image / input_scale + input_zero_point\n",
        "\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
        "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "\n",
        "    predictions[i] = output.argmax()\n",
        "\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "13myIyvtcyto"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(tflite_file, model_type):\n",
        "  global test_x\n",
        "  global test_y\n",
        "\n",
        "  test_image_indices = range(test_x.shape[0])\n",
        "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
        "\n",
        "  accuracy = (np.sum(test_y== predictions) * 100) / len(test_x)\n",
        "\n",
        "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
        "      model_type, accuracy, len(test_x)))"
      ],
      "metadata": {
        "id": "Lv1HJGcxcJR9"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(tflite_model_file, model_type=\"Float\")\n",
        "evaluate_model(tflite_model_quant_file, model_type=\"Quantized\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bac0X0MIcJ-y",
        "outputId": "43fead43-4303-4114-e63b-77245090341c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Float model accuracy is 100000.0000% (Number of test samples=10000)\n",
            "Quantized model accuracy is 100000.0000% (Number of test samples=10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "# Change this to test a different image\n",
        "test_image_index = 1\n",
        "\n",
        "## Helper function to test the models on one image\n",
        "def test_model(tflite_file, test_image_index, model_type):\n",
        "  global test_labels\n",
        "\n",
        "  predictions = run_tflite_model(tflite_file, [test_image_index])\n",
        "\n",
        "  plt.imshow(test_x[test_image_index])\n",
        "  template = model_type + \" Model \\n True:{true}, Predicted:{predict}\"\n",
        "  _ = plt.title(template.format(true= str(test_y[test_image_index]), predict=str(predictions[0])))\n",
        "  plt.grid(False)"
      ],
      "metadata": {
        "id": "wJ8BqBRecLa_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(tflite_model_file, test_image_index, model_type=\"Float\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "kgJlkK8kdnKT",
        "outputId": "fdb9cb5d-512e-41b1-a079-52546e71ea33"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(tflite_model_quant_file, test_image_index, model_type=\"Quantized\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "XWXNXX5ydpG5",
        "outputId": "e51be4b0-4135-49ff-d165-bfcc327a5d31"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 저장하는 2가지 방식\n",
        "#model.save('mobile_h5_model.h5')\n",
        "model.save('saved_model/mobile_model')"
      ],
      "metadata": {
        "id": "56tzoizBvvXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02ae145a-e9e0-41a8-9ef9-c0ddb1517152"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) MobilenetV3small_input with unsupported characters which will be renamed to mobilenetv3small_input in the SavedModel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/mobile_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/mobile_model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## int_16 Quantization 진행\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BsbJIZaRe4Up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 그냥 모델 변환\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()"
      ],
      "metadata": {
        "id": "f86tjs5cgQF2",
        "outputId": "a15b21d6-3875-4797-c30b-1cafa423ae1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) MobilenetV3small_input with unsupported characters which will be renamed to mobilenetv3small_input in the SavedModel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpq3n3ds2g/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpq3n3ds2g/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tflite_models_dir = pathlib.Path(\"/tmp/mobile_tflite_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)"
      ],
      "metadata": {
        "id": "w10u7Kb8gQJL"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tflite_model_file = tflite_models_dir/\"mobile_model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "metadata": {
        "id": "w53X2Rg_gQM4",
        "outputId": "3f8c7b0d-f887-471e-89e8-f954229e3b83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5997032"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]"
      ],
      "metadata": {
        "id": "Op0-J9PTgQO-"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input 에 대해서 변수 설정\n",
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(train_x).batch(1).take(100):\n",
        "    yield [input_value]"
      ],
      "metadata": {
        "id": "X_X0ETN8gQRe"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converter.representative_dataset = representative_data_gentflite_16x8_model = converter.convert()\n",
        "tflite_model_16x8_file = tflite_models_dir/\"mobile_model_quant_16x8.tflite\"\n",
        "tflite_model_16x8_file.write_bytes(tflite_16x8_model)"
      ],
      "metadata": {
        "id": "5WsVcqlQg99z",
        "outputId": "636eafcf-acd6-4057-a029-2eabe6bc5411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) MobilenetV3small_input with unsupported characters which will be renamed to mobilenetv3small_input in the SavedModel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpyth1ifpy/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpyth1ifpy/assets\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-4bf9dd8d85b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentative_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepresentative_data_gentflite_16x8_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtflite_model_16x8_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtflite_models_dir\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"mobile_model_quant_16x8.tflite\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtflite_model_16x8_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_16x8_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_and_export_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_conversion_params_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m     \u001b[0melapsed_time_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0mInvalid\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \"\"\"\n\u001b[0;32m-> 1210\u001b[0;31m     \u001b[0msaved_model_convert_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_as_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msaved_model_convert_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_convert_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_as_saved_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         return super(TFLiteKerasModelConverterV2,\n\u001b[0;32m-> 1193\u001b[0;31m                      self).convert(graph_def, input_tensors, output_tensors)\n\u001b[0m\u001b[1;32m   1194\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m       \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m     return self._optimize_tflite_model(\n\u001b[0;32m-> 1010\u001b[0;31m         result, self._quant_mode, quant_io=self.experimental_new_quantizer)\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Re-throws the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mConverterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_optimize_tflite_model\u001b[0;34m(self, model, quant_mode, quant_io)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0mq_allow_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allow_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         model = self._quantize(\n\u001b[0;32m--> 750\u001b[0;31m             model, q_in_type, q_out_type, q_activations_type, q_allow_float)\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m       \u001b[0mm_in_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_type\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0min_type\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_quantize\u001b[0;34m(self, result, input_type, output_type, activations_type, allow_float)\u001b[0m\n\u001b[1;32m    571\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentative_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m           \u001b[0mallow_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m           disable_per_channel=self._experimental_disable_per_channel)\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_is_unknown_shapes_allowed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Re-throws the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mConverterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/optimize/calibrator.py\u001b[0m in \u001b[0;36mcalibrate_and_quantize\u001b[0;34m(self, dataset_gen, input_type, output_type, allow_float, activations_type, resize_input, disable_per_channel)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         disable_per_channel)\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m   @convert_phase(Component.OPTIMIZE_TFLITE_MODEL,\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Quantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\nQuantization to 16x8-bit not yet supported for op: 'HARD_SWISH'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8loaafyrg-Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "onMTsU1Sg-DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_kClWazYg-GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "n7duY4Ggg-JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IsBXeF3Lg-Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1jzYjEMhg-Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0brvxh0kg-Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 구조까지 들어가 있는거\n",
        "#model_load = tf.keras.models.load_model('saved_model/my_model')"
      ],
      "metadata": {
        "id": "-n_RWZIAe4Uq"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input 에 대해서 변수 설정\n",
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(train_x).batch(1).take(100):\n",
        "    integer_16_image = np.array(input_value, dtype=np.float32)\n",
        "    yield [integer_16_image]"
      ],
      "metadata": {
        "id": "spHHwkvWe4Uq"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# int 8 로 quantization 진행하기\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT16]\n",
        "\n",
        "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
        "converter.inference_input_type = tf.uint16\n",
        "converter.inference_output_type = tf.uint16\n",
        "\n",
        "tflite_model_quant = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "eba73a72-e9de-4ff1-9ef8-cfa2100f868b",
        "id": "GNZ1USjGe4Ur"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-2154b20dcac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Ensure that if any ops can't be quantized, the converter throws an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupported_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpsSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLITE_BUILTINS_INT16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Set the input and output tensors to uint8 (APIs added in r2.3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/enum.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_member_map_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: TFLITE_BUILTINS_INT16"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그냥 파일 형태만 tflite로 변환 (float 형태임)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "tflite_model = converter.convert()"
      ],
      "metadata": {
        "id": "-gQed6J2e4Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
        "input_type = interpreter.get_input_details()[0]['dtype']\n",
        "print('input: ', input_type)\n",
        "output_type = interpreter.get_output_details()[0]['dtype']\n",
        "print('output: ', output_type)"
      ],
      "metadata": {
        "id": "x3zGfIc7e4Ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장 해주는 과정\n",
        "import pathlib\n",
        "\n",
        "tflite_models_dir = pathlib.Path(\"/tmp/mobile_tflite_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Save the unquantized/float model:\n",
        "tflite_model_file = tflite_models_dir/\"mobile_model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)\n",
        "# Save the quantized model:\n",
        "tflite_model_quant_file = tflite_models_dir/\"mobiile_model_quant.tflite\"\n",
        "tflite_model_quant_file.write_bytes(tflite_model_quant)"
      ],
      "metadata": {
        "id": "SuXD5Qs8e4Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tensorlffow lite 모델 실행 해보기"
      ],
      "metadata": {
        "id": "t1Vu_7vWe4Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tflite_model(tflite_file, test_image_indices):\n",
        "  global test_x\n",
        "\n",
        "  # Initialize the interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
        "  for i, test_image_index in enumerate(test_image_indices):\n",
        "    test_image = test_x[test_image_index]\n",
        "    test_label = test_y[test_image_index]\n",
        "\n",
        "    # Check if the input type is quantized, then rescale input data to uint8\n",
        "    if input_details['dtype'] == np.uint8:\n",
        "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "      test_image = test_image / input_scale + input_zero_point\n",
        "\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
        "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "\n",
        "    predictions[i] = output.argmax()\n",
        "\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "vxTXEOdme4Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(tflite_file, model_type):\n",
        "  global test_x\n",
        "  global test_y\n",
        "\n",
        "  test_image_indices = range(test_x.shape[0])\n",
        "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
        "\n",
        "  accuracy = (np.sum(test_y== predictions) * 100) / len(test_x)\n",
        "\n",
        "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
        "      model_type, accuracy, len(test_x)))"
      ],
      "metadata": {
        "id": "5df0fDkde4Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(tflite_model_file, model_type=\"Float\")\n",
        "evaluate_model(tflite_model_quant_file, model_type=\"Quantized\")"
      ],
      "metadata": {
        "id": "T3xB4Kwbe4Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "# Change this to test a different image\n",
        "test_image_index = 1\n",
        "\n",
        "## Helper function to test the models on one image\n",
        "def test_model(tflite_file, test_image_index, model_type):\n",
        "  global test_labels\n",
        "\n",
        "  predictions = run_tflite_model(tflite_file, [test_image_index])\n",
        "\n",
        "  plt.imshow(test_x[test_image_index])\n",
        "  template = model_type + \" Model \\n True:{true}, Predicted:{predict}\"\n",
        "  _ = plt.title(template.format(true= str(test_y[test_image_index]), predict=str(predictions[0])))\n",
        "  plt.grid(False)"
      ],
      "metadata": {
        "id": "I1dcgvGze4Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(tflite_model_file, test_image_index, model_type=\"Float\")"
      ],
      "metadata": {
        "id": "DDYgOPkWe4Uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(tflite_model_quant_file, test_image_index, model_type=\"Quantized\")"
      ],
      "metadata": {
        "id": "0Tqf4t7Pe4Uu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}